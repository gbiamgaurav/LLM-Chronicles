{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gbiamgaurav/LLM-Chronicles/blob/main/llm_chronicles_mlp_implementation_2_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Layer Perceptron Implementation #2.2\n",
        "\n",
        "In the initial segment of this notebook, we'll dive deep into the mechanics of a neural network, specifically focusing on a layer of a Multi-Layer Perceptron (MLP). Rather than approaching this with iterative loops, which can be computationally inefficient, we'll harness the power of vectorization. By leveraging linear algebra and matrix operations, we'll see how to efficiently compute the outputs of an MLP layer, setting the foundation for understanding more complex neural network architectures.\n",
        "\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/2.2%20-%20Lab%20-%20Multi-Layer%20Perceptron%20and%20MNIST%20/vectorization.png)"
      ],
      "metadata": {
        "id": "3q9O4F3FHzwj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RkV0_d1qGce"
      },
      "outputs": [],
      "source": [
        "# We're just getting started by importing the numpy library,\n",
        "# which is like a Swiss Army knife for numerical operations in Python.\n",
        "# We'll be using it a lot for our matrix operations.\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3x2 Weight matrix for this layer,\n",
        "# 2 input features, 3 output features (which means 3 perceptrons/units)\n",
        "WeightMatrix = np.array([\n",
        "    [1, 2],\n",
        "    [3, 4],\n",
        "    [5, 6]\n",
        "])\n",
        "\n",
        "# 2x1, 2 are the input features and 1 in the size of the batch\n",
        "Input_1 = np.array([\n",
        "    [0.15],\n",
        "    [0.25]\n",
        "])\n",
        "\n",
        "WeightMatrix, Input_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx8vKi5cqWSi",
        "outputId": "3dd4caa0-85c6-4bc8-b86c-b923e171d818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1, 2],\n",
              "        [3, 4],\n",
              "        [5, 6]]),\n",
              " array([[0.15],\n",
              "        [0.25]]))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix multiplication to perform the weighted sum of the input values\n",
        "# for each neuron in that layer\n",
        "np.matmul(WeightMatrix, Input_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6jUzYkJrMNT",
        "outputId": "43a7f6bf-5073-4cf4-e23b-7a8327171b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.65],\n",
              "       [1.45],\n",
              "       [2.25]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2x3, 2 are the input features and 3 in the size of the batch\n",
        "Input_1 = np.array([\n",
        "    [0.15, 0.3, 0.5],\n",
        "    [0.25, 0.4, 0.6]\n",
        "])\n",
        "np.matmul(WeightMatrix, Input_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0pdGbkX1evW",
        "outputId": "77d0d2b1-4675-4eef-d7d3-ee59e4c9b38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.65, 1.1 , 1.7 ],\n",
              "       [1.45, 2.5 , 3.9 ],\n",
              "       [2.25, 3.9 , 6.1 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reshaping input vector\n",
        "\n",
        "We'll now reshape our input data a bit. Instead of having our data in columns, we're now having them in rows. This is just another way to represent our data, which is more intutive and easier to wrok with in practice."
      ],
      "metadata": {
        "id": "4UgwPup72PHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3x2, 3 is batch size, 2 number of features\n",
        "Input_2 = np.array([\n",
        "    [0.15, 0.25],\n",
        "    [0.3, 0.4],\n",
        "    [0.5, 0.6]\n",
        "])\n",
        "\n",
        "Input_2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9SNrX2l_Kh6",
        "outputId": "8dd45871-258b-46b1-d539-78e3ab0e1c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This will fail as the row by columns multiplication\n",
        "# requires the second dimention of the first matrix\n",
        "# to be equal to the first dimension of the second.\n",
        "#np.matmul(WeightMatrix, Input_2)"
      ],
      "metadata": {
        "id": "WI0cInEN_ZmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WeightMatrix.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA29YPNdC5tU",
        "outputId": "c25435dc-7b36-4522-af95-a993c9ae6557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 3, 5],\n",
              "       [2, 4, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.matmul(Input_2, WeightMatrix.T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0g8uMZy_eB_",
        "outputId": "57574007-596b-44d6-dbcf-62119ff17470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.65, 1.45, 2.25],\n",
              "       [1.1 , 2.5 , 3.9 ],\n",
              "       [1.7 , 3.9 , 6.1 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Bias\n",
        "\n",
        "We can now add another vector for our layer which contains the bias of each neuron."
      ],
      "metadata": {
        "id": "aU2Ll34x9esJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bias = np.array([1.1, -2.1, -3.1])"
      ],
      "metadata": {
        "id": "oJBLZkWb9g-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.matmul(Input_2, WeightMatrix.T) + bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cc53232-7b8d-426f-d8c1-e2330e04522b",
        "id": "T2vNBJAB92wS"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.75, -0.65, -0.85],\n",
              "       [ 2.2 ,  0.4 ,  0.8 ],\n",
              "       [ 2.8 ,  1.8 ,  3.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding activation\n",
        "\n",
        "An activation function introduces non-linearity to the model, allowing it to learn from the error and make adjustments, which is essential for learning complex patterns.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/2.2%20-%20Lab%20-%20Multi-Layer%20Perceptron%20and%20MNIST/activations.png)\n",
        "\n",
        "Here we'll implement the ReLU (Rectified Linear Unit) function, defined as:\n",
        "\n",
        "$$\\text{ReLU}(x) =\n",
        "\\begin{cases}\n",
        "x & \\text{if } x > 0 \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}$$"
      ],
      "metadata": {
        "id": "bxHajsOJ-UGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "x = np.matmul(Input_2, WeightMatrix.T) + bias\n",
        "\n",
        "relu(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5E0qYlP3jAy",
        "outputId": "e1c1c1c8-1c91-460c-9716-14e0ab4d40cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.75, 0.  , 0.  ],\n",
              "       [2.2 , 0.4 , 0.8 ],\n",
              "       [2.8 , 1.8 , 3.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doing it all in PyTorch\n",
        "\n",
        "In the sections above, we delved deep into matrix operations using numpy. But why did we start there? Because at its core, deep learning is about matrix operations, and numpy provides a foundational understanding of these operations.\n",
        "\n",
        "![picture](https://upload.wikimedia.org/wikipedia/commons/9/96/Pytorch_logo.png?20211003060202)\n",
        "\n",
        "PyTorch is a leading deep learning framework developed by Facebook's AI Research lab. It offers a dynamic computational graph, which makes it particularly flexible for research. But at its heart, much of what PyTorch does is similar to our numpy operations, albeit optimized and with GPU support.\n",
        "\n",
        "In the next section, we'll transition from numpy to PyTorch, and you'll see that many concepts carry over seamlessly. The primary difference? With PyTorch, we're better equipped to build, train, and evaluate complex neural network models."
      ],
      "metadata": {
        "id": "S3Y59ipf-WzR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "KCzaOSzE6oaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear layer\n",
        "\n",
        "The PyTorch Linear layer is essentially an encapsulation of the matrix operations we explored with numpy. When you instantiate a Linear layer in PyTorch, you're creating a weight matrix (and an optional bias) under the hood. The forward pass of this layer performs a matrix multiplication with the input and the weight matrix, just like we did with numpy. So, when you think of a Linear layer, imagine it as an automated way of handling the weight matrix creation, management, and operations we manually executed earlier. In essence, the manual matrix multiplications we did using numpy are what the Linear layer in PyTorch does automatically, optimized, and at scale.\n"
      ],
      "metadata": {
        "id": "IKEst9KK7HfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = nn.Linear(2,3)"
      ],
      "metadata": {
        "id": "bIzPx8226tcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer.bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pQCN_3FNW8-",
        "outputId": "13abc58b-eae7-4029-d898-bc6f03ee56d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-0.1300,  0.7063,  0.6780], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer.weight.data = torch.tensor([\n",
        "    [1., 2.],\n",
        "    [3., 4.],\n",
        "    [5., 6.]\n",
        "])\n",
        "\n",
        "layer.weight\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1Sx2YTw658t",
        "outputId": "e4b0792e-f2d2-4afa-b57f-2032d22f5784"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[1., 2.],\n",
              "        [3., 4.],\n",
              "        [5., 6.]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer.bias.data = torch.tensor([1.1, -2.1, -3.1])\n"
      ],
      "metadata": {
        "id": "Wh44fL0J8qaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Input = torch.tensor([\n",
        "    [0.15, 0.25],\n",
        "    [0.3, 0.4],\n",
        "    [0.5, 0.6]\n",
        "])\n",
        "\n",
        "Input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJCF0lqP76Yu",
        "outputId": "5c787fac-37a6-42b2-c7fa-43551d357f8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1500, 0.2500],\n",
              "        [0.3000, 0.4000],\n",
              "        [0.5000, 0.6000]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer(Input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upD5GdYX8D5y",
        "outputId": "5aa0b5a1-106a-49c8-b603-cfcef3d27c46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.7500, -0.6500, -0.8500],\n",
              "        [ 2.2000,  0.4000,  0.8000],\n",
              "        [ 2.8000,  1.8000,  3.0000]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "x= layer(Input)\n",
        "F.relu(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf5cvyE_OG2A",
        "outputId": "7aea9aa6-f440-4798-ff70-94da38e4c560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.7500, 0.0000, 0.0000],\n",
              "        [2.2000, 0.4000, 0.8000],\n",
              "        [2.8000, 1.8000, 3.0000]], grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recognising digits\n",
        "\n",
        "In this section, we're diving into a hands-on exercise where we'll construct a Multi-Layer Perceptron (MLP) to recognize hand-written digits. We'll be using the MNIST dataset, a collection of 28x28 grayscale images of digits from 0 to 9. Often dubbed the \"Hello World\" of neural networks, the MNIST dataset is a foundational stepping stone in the world of deep learning. It provides a perfect balance of complexity and manageability, making it an ideal starting point for both beginners and experts looking to validate their models."
      ],
      "metadata": {
        "id": "COjsQxARxxb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and extract the dataset\n",
        "\n",
        "!wget https://github.com/kyuz0/mnist-png/raw/main/mnist.tgz\n",
        "!tar xzf mnist.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LExTgfuMx2jj",
        "outputId": "09f230b6-4995-4f9f-99d9-379d9b7ebcc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-23 14:24:08--  https://github.com/kyuz0/mnist-png/raw/main/mnist.tgz\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/kyuz0/mnist-png/main/mnist.tgz [following]\n",
            "--2023-09-23 14:24:08--  https://raw.githubusercontent.com/kyuz0/mnist-png/main/mnist.tgz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18294568 (17M) [application/octet-stream]\n",
            "Saving to: ‘mnist.tgz’\n",
            "\n",
            "mnist.tgz           100%[===================>]  17.45M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-09-23 14:24:11 (324 MB/s) - ‘mnist.tgz’ saved [18294568/18294568]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n"
      ],
      "metadata": {
        "id": "naHilON5z8Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'mnist/training/0/1.png'\n",
        "image = Image.open(image_path)\n",
        "image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 45
        },
        "id": "0CAQ0UKF0NwW",
        "outputId": "54413438-b043-48ec-d348-2bf7efcac32f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABD0lEQVR4nGP8z4AbMOGRo6Lk2QSmhHNw3n8UcF6AiYlJEMZDlTwpzcjEL8p07Cem5NfDckyMTCarmRhbIQLIdqbZP2FgYDj3xZ7hEoaDzm79/9+u57+EYd7//+gOOi/AxOT9eXPrq///GbnPotp5M5JRVG81hM3IFIki+cOHiW/7m8cwSWsUyWNMTAfgNqBLWjA6IPzEwGiF7JUtFxj9Ee5mZDRAdu1KJolnMH0/yhldPiMbu5JJHi5XzSS7/T+qZC7Mu5FMgXC7oZKMshBGrwBjzH90SSbWnPOPVvrIMsqHH8eUZGKSVGdiYrKqQfgIJvnYnImJkYlJNPf/f0zJ/8/qmBiZCm+hyP1nHIB0CwAm9nC3iHlH3AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert image into tensor\n",
        "\n",
        "The initial step in our journey involves transforming the PNG images into a more digestible format for our neural network. Specifically, we'll convert these images into matrices or tensors, where each entry corresponds to a pixel in the image. These numerical values represent the brightness or intensity of each pixel, ranging from 0 (complete darkness) to 255 (full brightness). By representing our images as matrices of pixel intensities, we can feed them into our neural network, allowing it to learn and recognize patterns within the hand-written digits.\n",
        "\n",
        "# ![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/2.2%20-%20Lab%20-%20Multi-Layer%20Perceptron%20and%20MNIST%20/input.png)"
      ],
      "metadata": {
        "id": "F86N4aN89ACP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Apply the transformation to the image\n",
        "tensor_image = transform(image).squeeze(0)\n",
        "\n",
        "# Check the shape of the tensor\n",
        "print(tensor_image.shape)\n",
        "\n",
        "tensor_image[20][22]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AcFwRI90V4i",
        "outputId": "c91332dc-dea7-4b78-e153-e93005062c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([28, 28])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6qmEO7ZPLnW",
        "outputId": "135aecac-a450-4d5a-c85a-26190a55450f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8000, 0.3725, 0.0039,\n",
              "         0.3725, 0.8039, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8118, 0.0627, 0.0078, 0.0078,\n",
              "         0.0078, 0.0667, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 0.7882, 0.1059, 0.0039, 0.0078, 0.0588,\n",
              "         0.0824, 0.0078, 0.7765, 0.9765, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 0.9608, 0.7647, 0.1176, 0.0078, 0.0039, 0.0078, 0.2039,\n",
              "         0.6706, 0.0078, 0.0039, 0.5216, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 0.3569, 0.0078, 0.0078, 0.0078, 0.0039, 0.0078, 0.0078,\n",
              "         0.6235, 0.2549, 0.0039, 0.3412, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 0.8000, 0.0627, 0.0039, 0.0039, 0.2510, 0.5529, 0.0039, 0.1020,\n",
              "         0.8157, 0.6863, 0.0000, 0.3373, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         0.8118, 0.0627, 0.0078, 0.0078, 0.2941, 0.9529, 0.7020, 0.5216, 0.9176,\n",
              "         1.0000, 1.0000, 0.0039, 0.0431, 0.8039, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8510,\n",
              "         0.3490, 0.0039, 0.0824, 0.1804, 0.6706, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 0.0039, 0.0078, 0.3490, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9725, 0.2980,\n",
              "         0.0078, 0.0549, 0.7176, 0.9255, 0.8902, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 0.0039, 0.0078, 0.2314, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7765, 0.0078,\n",
              "         0.0078, 0.7529, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 0.0039, 0.0078, 0.2314, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.2196, 0.0039,\n",
              "         0.2510, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 0.0000, 0.0039, 0.2275, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7020, 0.0314, 0.0078,\n",
              "         0.5608, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 0.0039, 0.0078, 0.4157, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6627, 0.0078, 0.0941,\n",
              "         0.9020, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         0.9725, 0.4667, 0.0039, 0.2667, 0.9529, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6627, 0.0078, 0.1216,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9725,\n",
              "         0.4824, 0.0078, 0.1137, 0.7176, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6627, 0.0078, 0.4275,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8118, 0.3490,\n",
              "         0.0078, 0.3176, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6627, 0.0039, 0.1137,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5529, 0.0627, 0.0039,\n",
              "         0.3608, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6627, 0.0078, 0.0196,\n",
              "         0.4235, 0.8118, 0.8863, 0.6627, 0.2980, 0.1137, 0.0039, 0.1216, 0.3412,\n",
              "         0.7804, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6627, 0.0078, 0.0078,\n",
              "         0.0078, 0.0980, 0.1529, 0.0078, 0.0078, 0.0078, 0.2275, 0.4863, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8902, 0.2157, 0.0078,\n",
              "         0.0078, 0.0039, 0.0078, 0.0078, 0.0824, 0.4275, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9020, 0.4941,\n",
              "         0.0078, 0.0039, 0.0078, 0.4431, 0.8549, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000],\n",
              "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def imshow(tensor_image):\n",
        "    plt.imshow(tensor_image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display the tensor image using matplotlib\n",
        "imshow(tensor_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "ldJ4f5ko1ANC",
        "outputId": "f2bfd6b2-4d11-4f77-d47b-e8fd2182bd42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model\n",
        "\n",
        "Here, we're defining our neural network model for the MNIST dataset.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/2.2%20-%20Lab%20-%20Multi-Layer%20Perceptron%20and%20MNIST%20/mpl.png)\n",
        "\n",
        "*   **Layer 1**: This layer has 28x28 input features, corresponding to the pixels of the MNIST images. It contains 100 units or neurons, and we apply the ReLU activation function to introduce non-linearity.\n",
        "\n",
        "*   **Layer 2**: This hidden layer has 100 input features from the previous layer and outputs 150 activations for the next layer. We apply the ReLU activation as well for this layer.\n",
        "\n",
        "*   **Output layer**: Layer 2 connects into this final output layer with 10 units, representing the 10 digits (0-9) we aim to classify. Notice we don't use an activation on the **logits** output by this layer.\n"
      ],
      "metadata": {
        "id": "646Z-lLn9vmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class MNISTNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.Layer1 = nn.Linear(28*28, 100)\n",
        "    self.Layer2 = nn.Linear(100, 150)\n",
        "    self.Layer3 = nn.Linear(150, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x =  F.relu(self.Layer1(x))\n",
        "    x =  F.relu(self.Layer2(x))\n",
        "    x =  self.Layer3(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "GBmlgB0_B_rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MNISTNet()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jZ7SgcrC8_O",
        "outputId": "104c883f-da6d-4c2a-c437-7d75f3ac95d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MNISTNet(\n",
              "  (Layer1): Linear(in_features=784, out_features=100, bias=True)\n",
              "  (Layer2): Linear(in_features=100, out_features=150, bias=True)\n",
              "  (Layer3): Linear(in_features=150, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"Name: {name}\")\n",
        "    print(f\"Value: {param.data}\")\n",
        "    print(\"------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zddOsP5MDlBC",
        "outputId": "1fbdfd98-49a7-4873-a0e5-2c753501feee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Layer1.weight\n",
            "Value: tensor([[-0.0139, -0.0344, -0.0199,  ..., -0.0211,  0.0353,  0.0151],\n",
            "        [ 0.0040, -0.0198, -0.0183,  ..., -0.0288,  0.0157,  0.0196],\n",
            "        [ 0.0237, -0.0293,  0.0017,  ..., -0.0073,  0.0090, -0.0320],\n",
            "        ...,\n",
            "        [-0.0017, -0.0129, -0.0062,  ..., -0.0018, -0.0121, -0.0116],\n",
            "        [ 0.0333, -0.0089, -0.0270,  ..., -0.0116,  0.0013,  0.0291],\n",
            "        [ 0.0279, -0.0016, -0.0223,  ...,  0.0281,  0.0104, -0.0107]])\n",
            "------\n",
            "Name: Layer1.bias\n",
            "Value: tensor([ 0.0084, -0.0315,  0.0074,  0.0312,  0.0090, -0.0083, -0.0153, -0.0118,\n",
            "         0.0019, -0.0177,  0.0251,  0.0280, -0.0174, -0.0275,  0.0167, -0.0093,\n",
            "        -0.0046, -0.0015,  0.0016,  0.0036, -0.0112,  0.0330, -0.0103,  0.0098,\n",
            "        -0.0031, -0.0292, -0.0129, -0.0345,  0.0144, -0.0003,  0.0021,  0.0047,\n",
            "         0.0311, -0.0060, -0.0153, -0.0207, -0.0066,  0.0203, -0.0266,  0.0246,\n",
            "        -0.0026,  0.0269,  0.0336, -0.0036, -0.0342, -0.0128, -0.0018, -0.0102,\n",
            "        -0.0084,  0.0307,  0.0098,  0.0201,  0.0118,  0.0024, -0.0022, -0.0122,\n",
            "         0.0034, -0.0311,  0.0021,  0.0272,  0.0118, -0.0095, -0.0080, -0.0237,\n",
            "         0.0199, -0.0277, -0.0317, -0.0196,  0.0175,  0.0131,  0.0206, -0.0068,\n",
            "         0.0211,  0.0101, -0.0275,  0.0320, -0.0228,  0.0232,  0.0001,  0.0169,\n",
            "         0.0048, -0.0054, -0.0046,  0.0241, -0.0174, -0.0317,  0.0270, -0.0332,\n",
            "         0.0062,  0.0214, -0.0199, -0.0250,  0.0327,  0.0335, -0.0232,  0.0010,\n",
            "        -0.0208,  0.0203,  0.0035, -0.0096])\n",
            "------\n",
            "Name: Layer2.weight\n",
            "Value: tensor([[-0.0947, -0.0195,  0.0414,  ..., -0.0132,  0.0318, -0.0198],\n",
            "        [ 0.0572,  0.0860,  0.0266,  ..., -0.0489, -0.0138,  0.0272],\n",
            "        [ 0.0864, -0.0382, -0.0844,  ...,  0.0806, -0.0364, -0.0267],\n",
            "        ...,\n",
            "        [ 0.0512,  0.0496,  0.0154,  ...,  0.0923, -0.0460, -0.0720],\n",
            "        [-0.0145,  0.0265, -0.0222,  ...,  0.0808,  0.0974,  0.0943],\n",
            "        [-0.0016,  0.0400,  0.0218,  ...,  0.0859,  0.0151, -0.0675]])\n",
            "------\n",
            "Name: Layer2.bias\n",
            "Value: tensor([ 0.0132,  0.0483, -0.0109, -0.0321, -0.0324, -0.0872, -0.0104,  0.0708,\n",
            "         0.0708,  0.0761, -0.0239, -0.0511,  0.0019, -0.0013, -0.0325,  0.0163,\n",
            "         0.0363,  0.0333,  0.0920, -0.0010, -0.0049,  0.0761,  0.0051, -0.0942,\n",
            "        -0.0594,  0.0286,  0.0451,  0.0321,  0.0540,  0.0900, -0.0871, -0.0846,\n",
            "         0.0085, -0.0920,  0.0205,  0.0088, -0.0024,  0.0221,  0.0781,  0.0454,\n",
            "         0.0973,  0.0017,  0.0936,  0.0105,  0.0946,  0.0197,  0.0052, -0.0800,\n",
            "         0.0797,  0.0961,  0.0152,  0.0612,  0.0602, -0.0897,  0.0826,  0.0365,\n",
            "        -0.0554,  0.0016, -0.0393,  0.0967, -0.0351, -0.0589,  0.0592,  0.0011,\n",
            "         0.0937,  0.0891, -0.0094, -0.0896, -0.0241,  0.0305, -0.0885, -0.0761,\n",
            "        -0.0757,  0.0557, -0.0358,  0.0234, -0.0775, -0.0804,  0.0461,  0.0612,\n",
            "        -0.0878, -0.0758,  0.0095,  0.0664, -0.0174,  0.0768,  0.0217, -0.0195,\n",
            "        -0.0605,  0.0281,  0.0980, -0.0064,  0.0035,  0.0865,  0.0399,  0.0220,\n",
            "        -0.0512,  0.0009, -0.0501,  0.0780, -0.0564, -0.0956, -0.0788, -0.0369,\n",
            "        -0.0574,  0.0026, -0.0025, -0.0668,  0.0877, -0.0130, -0.0918,  0.0685,\n",
            "        -0.0902,  0.0988,  0.0955, -0.0499,  0.0846, -0.0823, -0.0004,  0.0708,\n",
            "         0.0283,  0.0244, -0.0827, -0.0510, -0.0453,  0.0857,  0.0573, -0.0763,\n",
            "        -0.0675,  0.0357,  0.0731, -0.0548,  0.0828,  0.0438, -0.0646,  0.0023,\n",
            "         0.0486, -0.0966, -0.0984, -0.0772, -0.0911,  0.0320,  0.0592, -0.0057,\n",
            "         0.0711, -0.0542, -0.0660, -0.0934, -0.0155, -0.0607])\n",
            "------\n",
            "Name: Layer3.weight\n",
            "Value: tensor([[-0.0308, -0.0363,  0.0659,  ..., -0.0309,  0.0793,  0.0116],\n",
            "        [-0.0660,  0.0597, -0.0476,  ..., -0.0708, -0.0534, -0.0309],\n",
            "        [ 0.0754,  0.0265, -0.0373,  ...,  0.0161, -0.0748,  0.0114],\n",
            "        ...,\n",
            "        [ 0.0203, -0.0672, -0.0147,  ...,  0.0702,  0.0610, -0.0251],\n",
            "        [ 0.0422,  0.0279, -0.0693,  ..., -0.0070, -0.0660, -0.0342],\n",
            "        [-0.0432,  0.0399,  0.0802,  ...,  0.0332, -0.0187, -0.0695]])\n",
            "------\n",
            "Name: Layer3.bias\n",
            "Value: tensor([ 0.0732, -0.0361,  0.0630, -0.0478,  0.0224,  0.0371, -0.0337, -0.0286,\n",
            "        -0.0340, -0.0500])\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.Layer3.weight.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DO5-6wMEf00",
        "outputId": "1c82f070-fba8-4b0e-d98c-659037500fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 150])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward pass\n",
        "Let's now take a single image, reshape it into an input tensor, with the first dimention being the batch size of 1 and feed it to the MLP. This will output the logits that we can turn into probabilities using the **softmax** function.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/2.2%20-%20Lab%20-%20Multi-Layer%20Perceptron%20and%20MNIST%20/multi-class.png)"
      ],
      "metadata": {
        "id": "SCVkhXemB3st"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_image.reshape(1,28*28)"
      ],
      "metadata": {
        "id": "ankNqSe7EBg9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "008f785e-581d-448c-e92d-db301cf58f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 0.8000, 0.3725, 0.0039, 0.3725, 0.8039, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 0.8118, 0.0627, 0.0078, 0.0078, 0.0078, 0.0667, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 0.7882, 0.1059, 0.0039, 0.0078, 0.0588, 0.0824, 0.0078, 0.7765,\n",
              "         0.9765, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         0.9608, 0.7647, 0.1176, 0.0078, 0.0039, 0.0078, 0.2039, 0.6706, 0.0078,\n",
              "         0.0039, 0.5216, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 0.3569, 0.0078, 0.0078, 0.0078, 0.0039, 0.0078, 0.0078, 0.6235,\n",
              "         0.2549, 0.0039, 0.3412, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 0.8000, 0.0627, 0.0039, 0.0039, 0.2510, 0.5529, 0.0039, 0.1020,\n",
              "         0.8157, 0.6863, 0.0000, 0.3373, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 0.8118, 0.0627, 0.0078, 0.0078, 0.2941, 0.9529, 0.7020, 0.5216,\n",
              "         0.9176, 1.0000, 1.0000, 0.0039, 0.0431, 0.8039, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 0.8510, 0.3490, 0.0039, 0.0824, 0.1804, 0.6706, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 0.0039, 0.0078, 0.3490, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 0.9725, 0.2980, 0.0078, 0.0549, 0.7176, 0.9255, 0.8902, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0039, 0.0078, 0.2314, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 0.7765, 0.0078, 0.0078, 0.7529, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0039, 0.0078, 0.2314,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 0.2196, 0.0039, 0.2510, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0000, 0.0039,\n",
              "         0.2275, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 0.7020, 0.0314, 0.0078, 0.5608, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.0039,\n",
              "         0.0078, 0.4157, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 0.6627, 0.0078, 0.0941, 0.9020, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9725, 0.4667,\n",
              "         0.0039, 0.2667, 0.9529, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6627, 0.0078, 0.1216, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9725, 0.4824,\n",
              "         0.0078, 0.1137, 0.7176, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6627, 0.0078, 0.4275,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8118, 0.3490,\n",
              "         0.0078, 0.3176, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6627, 0.0039,\n",
              "         0.1137, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.5529, 0.0627,\n",
              "         0.0039, 0.3608, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.6627,\n",
              "         0.0078, 0.0196, 0.4235, 0.8118, 0.8863, 0.6627, 0.2980, 0.1137, 0.0039,\n",
              "         0.1216, 0.3412, 0.7804, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         0.6627, 0.0078, 0.0078, 0.0078, 0.0980, 0.1529, 0.0078, 0.0078, 0.0078,\n",
              "         0.2275, 0.4863, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 0.8902, 0.2157, 0.0078, 0.0078, 0.0039, 0.0078, 0.0078, 0.0824,\n",
              "         0.4275, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 0.9020, 0.4941, 0.0078, 0.0039, 0.0078, 0.4431,\n",
              "         0.8549, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = model(tensor_image.reshape(1,28*28))"
      ],
      "metadata": {
        "id": "WUYgj1qkDsnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgJMWjH2RhDp",
        "outputId": "bd85d38b-1f49-47db-8433-2350f2c012c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1321, -0.0515,  0.1063, -0.2405, -0.0755,  0.0825, -0.0529,  0.1211,\n",
              "         -0.0676, -0.0296]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.softmax(y_hat, dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF9G9IHEEFAr",
        "outputId": "07514be7-824d-4ec1-f8c9-1d4225fff930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1143, 0.0951, 0.1114, 0.0787, 0.0929, 0.1088, 0.0950, 0.1130, 0.0936,\n",
              "         0.0972]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using GPUs\n",
        "\n",
        "One of the standout features of PyTorch is its seamless GPU integration. In this cell, we'll demonstrate how effortlessly you can move your model to the GPU. By doing so, you can harness the power of parallel processing, which GPUs are optimized for, leading to significantly faster computations especially during training. This is particularly beneficial for large-scale neural networks and datasets.\n",
        "\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/kyuz0/llm-chronicles/main/2.2%20-%20Lab%20-%20Multi-Layer%20Perceptron%20and%20MNIST%20/gpus.png)"
      ],
      "metadata": {
        "id": "IOT7yaj0CqH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE"
      ],
      "metadata": {
        "id": "y8TMQjRR_DGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b2b2b6f-669c-4ab8-9b8b-0206113b5d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(DEVICE)\n",
        "\n",
        "y_hat = model(tensor_image.reshape(1,28*28).to(DEVICE))"
      ],
      "metadata": {
        "id": "tfjXOkPuDVhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIXKEzixSau1",
        "outputId": "e50eb916-0226-4464-e9f3-64aed7edb2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1321, -0.0515,  0.1063, -0.2405, -0.0755,  0.0825, -0.0529,  0.1211,\n",
              "         -0.0676, -0.0296]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}